---
layout: post
title: "Does model averaging make sense?"  
date: `r Sys.time()`
published: false
tags: [model selection, r, ecology]
---

Brian Cade [published a scathing condemnation of current statistical practices in ecology.](http://onlinelibrary.wiley.com/doi/10.1890/14-1639.1/full) It promises to be highly influential; I have seen it cited by reviewers already. I agree with a great many points Brian raised. I also disagree with one very central point. 

First let's deal with the common ground. Brian's assessment of how carelessly AIC~c~ based model averaging is being used by ecologists is spot on. There's a lot of sloppy reporting of results as well as egregiously misinformed conclusions. Perhaps the biggest issue with AIC~c~ approaches is that they require *thinking* to be used effectively. In my experience with teaching ecologists to analyze data, they are desperate to avoid thinking, and it's close cousin, judgement. They want a turn key analysis with a clear, unambiguous interpretation. Unfortunately that's not what AIC~c~ gives you. 

Issues where we agree:

* Multi-collinearity among predictors is bad. Really bad. Worse than I thought.
* model averaging including interaction terms is problematic.
* using AIC weights to measure relative importance of predictors is a lousy idea.
* using AIC model averaged coefficients to make predictions is just wrong. 

Where I disagree with Brian is whether it makes sense to model averge regression coefficients at all. Brian's central point is that any degree of multi-collinearity between predictors causes the units of the regression coefficients to change depending on what other predictors are in the model. It is certainly true that the magnitude and even sign of a regression coefficient can change depending on which other predictors are in the model (this is the [Frish-Waugh-Lovell Theorem](https://en.wikipedia.org/wiki/Frisch–Waugh–Lovell_theorem)). Brian's assertion is that the *units* of the regression coefficients change between models as a result of this effect, and that means it is mathematically inappropriate to combine estimates from different models in an average. 
The consequences of the theorem are clearly correct; it is this latter interpretation of those effects that I disagree with. 

As Brian did, I will start by examining the basic linear regression equation. 

$$
\begin{aligned}
E[y|x] = & X \beta + \epsilon\\
E[y|x] = & X_1 \beta_1 + X_c \beta_c + \epsilon
\end{aligned}
$$

The second equation partitions the predictors into a focal predictor $X_1$ and everything else, $X_c$. $\beta_1$ is a 1 x 1 matrix of the regression coefficient for $X_1$, and $\beta_c$ is a vector of regression coefficients for all the other predictors in $X_c$. When I look at that equation, it seems to me that the units of the products $X_1\beta_1$ and $X_c\beta_c$ have to be units of $y$. If that's the case, then the units of $\beta_1$ are $y / X_1$. This might be the place where there is some deeper mathematics going on, so I am happy to be corrected, but as far as I can see the units of $\beta_1$ are independent of whatever $X_c$ is. 

Next, Brian lays out several versions of the equation for $\beta_1$ to show how it is influenced by $X_c$. Maybe this is where the units change.

$$
\begin{aligned}
\beta_{1,j} = 
\end{aligned}
$$

