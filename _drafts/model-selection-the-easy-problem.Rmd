--- 
layout: post 
title:  Model selection and the art of evidence II 
published: false 
tags: [statistics, model selection] 
bibliography: references.bib
---

```{r setup, echo=FALSE, include=FALSE}
# load necessary packages here
library(xtable)
library(dplyr)
library(tidyr)
library(broom)
library(purrr)
library(ggplot2)
library(leaps)
library(MASS)
# need to create this here.
baseline=c(0.5,0.3667,0.233,0.1,0)

```

Imagine a situation with a single, continuous response variable and 5 continuous, independent predictor variables. The predictor variables can be scaled and centered, so can be assumed to be normally distributed with a mean of zero and a variance of one. The relationship between the predictors and the response is always linear and there are no interactions. In this circumstance I can always rank the predictor variables from greatest effect to least effect, and I will number the predictor variables from strongest to weakest. I scale the "effect size" of each predictor relative to the residual error in the model $\sigma_{error}$, for example $\beta_1 = c_1\sigma_{error}$, where $\beta_i$ is the coefficient of the $i^{th}$ predictor. If the response has also been centered and scaled, then the intercept $\beta_0$ will be zero and the residual error will be one. With these definitions, a "scenario" consists only of a vector of $c_i$'s. The baseline scenario will be `r print(baseline,digits=2)`, which means the true model consists of only the first four variables.[^allthecode]

```{r easy1,echo=FALSE,fig.cap='A sample dataset from the baseline scenario with $N=2m=64$.'}
set.seed(123456)

make.data=function(C=c(2,1,0.5,0.25,0),N,re = 1){
X = matrix(rnorm(length(C)*N),nrow=N)
Ypred = X %*% C
Y = rnorm(N,Ypred,re)
data=data.frame(Y,X)
names(data)[2:(length(C)+1)] = paste("X.",1:length(C),sep="")
return(data)
}
m = 32
N=2*m

data1 = make.data(baseline,N=N)
pairs(data1,pch=19,col=rgb(0.2,0.2,0.2,0.2))
``` 

A single sample from the baseline scenario has clear relationships between $Y$ and $X_1$ (Figure ). Fitting a linear model with all 5 variables is more revealing, but even then only the first 3 variables have clear effects ([Table 2](#lm0)). 

```{r simplelm, echo=FALSE, results='asis'}
data1.lm0 = lm(Y~X.1+X.2+X.3+X.4+X.5,data=data1)
lm0.table = xtable(data1.lm0,caption="Output of a linear model with all 5 predictor variables included for a single sample from the baseline scenario.",label="lm0")
print(lm0.table,type = "html", caption.placement="top", html.table.attributes = "id=lm0 border=1")
```

There are $m=32$ ($2^5=32$) possible models with 5 linear effect variables. In hypothesis testing mode, the goal is to accurately identify the true model out of the set of possible models. In fact, using AIC does not require that the "true" model is in the set of possible models [@burnham2003model], but this is the easy problem, after all. Measuring the quality of a model selection procedure could be done is several ways. The best metric depends on the goal. For hypothesis testing and AIC we might use the weight of the true model; the frequency with which the true model is the AIC best model is another. Both of these only work if the true model is in the set of possible models. Another metric is model diversity

$$
H_{model}=\sum_{i=1}^m -w_i log(w_i)
$$

which is just Shannon-Wiener information calculated using the weights of the models. $e^{H_{model}}$ will have a minimum value of 1 and a maximum value of $m$. This measures the degree of certainty in the conclusion drawn from the model set. 

In hypothesis testing we are concerned with the rejecting the null hypothesis $\beta_i = 0$.  Dupont and Balmer [-@Dupont1998589] provided formulas to calculate the [power of rejecting the hypothesis that $\beta_1 = 0$ for single linear regression](#fig:powerCurve). This power is only an upper bound for the baseline case which includes the effects of other variables as well. The addition of other variables means that the residual error is actually larger, and hence the power lower, if I fit a single variable model to the same data.     

```{r powerCurve,echo=FALSE, fig.cap="Power $(1-\\beta)$ to reject the hypothesis that $\\beta_1 = 0$ for different effect sizes and sample sizes of $m$, $2m$, and $3m$. The red dots indicate the effect sizes in the baseline scenario."}
v = N-2
delta = seq(0,.5,0.01)
power = pt(delta*sqrt(N)-qt(0.975,v),v) + pt(-delta*sqrt(N)-qt(0.975,v),v)
plot(delta,power,xlab="Effect size",ylab=expression(1-beta),type="l",lwd=3)
power = pt(baseline*sqrt(N)-qt(0.975,v),v) + pt(-baseline*sqrt(N)-qt(0.975,v),v)
points(baseline,power,pch=19,col="red",cex=2)
v = (N/2)-2
power = pt(delta*sqrt(N/2)-qt(0.975,v),v) + pt(-delta*sqrt(N/2)-qt(0.975,v),v)
lines(delta,power,lty=2,lwd=3)
v = (3*N/2)-2
power = pt(delta*sqrt(3*N/2)-qt(0.975,v),v) + pt(-delta*sqrt(3*N/2)-qt(0.975,v),v)
lines(delta,power,lty=3,lwd=3)
legend("topleft",bty="n",legend=c("m","2m","3m"),title="Sample Size",lty=c(2,1,3),lwd=3)
``` 

So, another way to evaluate the ability of a model selection procedure is to calculate the frequency with which a particular null hypothesis is rejected. 

The metrics discussed above do not tell us whether that conclusion is biased however -- I may be very certain about the wrong model. Bias is important when the goal is prediction or estimation. For the prediction goal I will look at a calibration curve -- a plot of the fitted values from a model or models against the observations. If all is well, this curve should be a straight line with a slope of one and an intercept of zero. Standardizing the intercept and slope of the calibration curve with the estimated standard errors will provide two metrics that should have means of zero and variances of one if the model selection procedure is producing unbiased predictions. Bias is also an issue for estimation, but in that case we are concerned about the estimated effect differing systematically from the true effect. We can estimate this bias simply as the average difference between the estimated coefficient and the true value.

Finally, I am interested in the precision of the estimated effects and predictions. In general, a model selection procedure that produces more precise estimates and predictions is preferred. However, I also worry about "coverage" -- how does the probability of the true value falling in a confidence interval compare to the nominal probability? 

I'll use five different model selection approaches for each goal. The first is the simplest. Following the recommendation of Harrell [-@Harrell2001] I will simply use the full model with all variables considered. The second procedure is backwards selection; starting with the full model remove terms with the smallest F ratios until all remaining terms have significant F ratios when deleted. For the third and fourth methods I will use AIC in two different ways. The third approach will simply use the AIC selected top model only. Fourth, I will use the entire model set to test hypotheses, make predictions, and estimate coefficients. Finally, I will follow a suggestion of Ben Bolker and explore the use of shrinkage methods like the lasso. 
### Hypothesis testing

#### Full model

For hypothesis testing I want to know how often I reject a null hypothesis of interest. The simplest null is $\beta_i = 0$. I'm also interested in the overall F test -- the probability that at least one of the predictors is significant.

```{r testPower1, echo=FALSE, eval=TRUE}
set.seed(234561)
results = matrix(NA,nrow=1000,ncol=7)
for (i in 1:1000){
  test.lm = lm(Y~X.1+X.2+X.3+X.4+X.5,data=make.data(baseline,N=N))
  results[i,1:6] = tidy(test.lm)$statistic
  results[i,7] = glance(test.lm)$statistic
}
simPower <- apply(results[,1:6],2,function(x)sum(x>qt(0.975,N-2)))
# compare to 
v = N-2
power = pt(baseline*sqrt(N)-qt(0.975,v),v) + pt(-baseline*sqrt(N)-qt(0.975,v),v)
expectedPower <- c(25,floor(power*1000))
knitr::kable(cbind(simPower,expectedPower))
``` 

So power for the individual coefficients is pretty good compared to the expectation. The simulated values are a bit low because the expectation is calculated assuming there is a single effect in the model. The overall model was significant `r sum(results[,7]>qf(0.95,6,N-6))` times out of 1000. 

It is worth repeating this exercise for a range of sample sizes. Overall power was excellent for $N = 2m$. So I'll repeat the process for sample sizes from 10 up to $2m$. I'm fitting 6 coefficients, so anything less than 30 is, frankly, ridiculous. But let's see what we get.

```{r samplesizePower, eval=FALSE, cache = TRUE}
# first prepare a dataframe with the inputs
df <- crossing(N=seq(10,2*m,5),Rep=1:1000)
df <- mutate(df,
             data = map(N, make.data, C=baseline),
             fullfit = map(data, function(xx)lm(Y~X.1+X.2+X.3+X.4+X.5, data=xx)))
df2 <- mutate(df, 
             statistics = map(fullfit, glance),
             estimates = map(fullfit, tidy)) %>% 
  unnest(statistics)
ggplot(df2, aes(x = factor(N), y = p.value)) + geom_boxplot() + geom_hline(yintercept = 0.05, linetype = 2)
```

This is essentially the inverse of the power curve for the overall model. Somewhere around 5 times the number of estimated parameters gives a median *p* less than 0.05. That's not as bad as I expected. 

#### Backwards selection

Now I will use backwards selection to simplify each model prior to testing the hypotheses for each coefficient and the overall model. I need to write a little function to automate backwards selection using F tests. There are other tools out there (e.g. package leaps) but they all seem to use AIC or other information criteria.

```{r testPowerBS}
set.seed(234561)
results = matrix(NA,nrow=1000,ncol=7)
autodrop <- function(fullmodel){
  keepgoing <- TRUE
  currentmodel <- fullmodel
  currentvars <- all.vars(formula(fullmodel))
  while(keepgoing){
    drop1out <- drop1(currentmodel, test="F")
    if (max(drop1out$`Pr(>F)`, na.rm = TRUE) < 0.05 | length(currentvars)<2){
      keepgoing <- FALSE
    } else {
      dropvar <- which.max(drop1out$`Pr(>F)`)
      currentFormula <- formula(currentmodel)
      currentVars <- all.vars(currentFormula)
      newFormula <- paste(currentVars[c(-1, -dropvar)], collapse="+")
      if (nchar(newFormula)>0){
        newFormula <- paste0("Y~", newFormula)
      } else {
        # dropped all variables
        newFormula <- "Y~1"
        keepgoing <- FALSE
      }
      currentmodel <- update(currentmodel, newFormula)
    }
  }
  return(currentmodel)
}

BSresults <- replicate(1000, {
  test.lm = lm(Y~X.1+X.2+X.3+X.4+X.5,data=make.data(baseline,N=N))
  autodrop(test.lm)
})


simPower <- apply(results[,1:6],2,function(x)sum(x>qt(0.975,N-2)))
# compare to 
v = N-2
power = pt(baseline*sqrt(N)-qt(0.975,v),v) + pt(-baseline*sqrt(N)-qt(0.975,v),v)
expectedPower <- c(25,floor(power*1000))
knitr::kable(cbind(simPower,expectedPower))

```


```{r eval=FALSE, echo=FALSE}
# should be 1,2,3,4,5 
table(apply(apply(results[,2:6],1,order,decreasing=TRUE),2,paste,sep="",collapse=""))
# how many times is strongest variable in each position?
table(apply(apply(results[,2:6],1,order,decreasing=TRUE),2,function(x)which(x==1)))

```

```{r easyAIC,echo=FALSE}
``` 

[^allthecode]: All the code for this post, including that not shown, [can be found here](https::/github.com/atyre2/atyre2.github.io/blob/master/_drafts/model-selection-the-easy-problem.Rmd).

## Literature cited
